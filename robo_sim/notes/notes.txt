Effects on knowledge gain:
	Minimum size of history - more random actions to start learning from
	Maximum size of history - size of corpus of data, larger seems for stable but slows learning
	minibatch size - more of history is sampled
	discount 
	* all of these effect the rate at which knowledge it picked up and how that knowledge is maintained

	* there seems to be a delicate balance between the value of food, penalty of death, history length and collision penalty

	* so far, any increase in sensor count over 3, makes the bot worse
	* small nets are better 
	* every bot hits a peak and then it's behavior degenerates to spinning in a circle somewhere relatively far from food.
		* could be that wall penalties are forcing it away from everything




TODO:
	1. Fix git crap
	2. implement good way to experiment w/ models (ABC for dqn?)
	3. write out model files periodically
	4. Seed models w/ example? implement a manual record mode
		* write all of the actions and results out to a file, read from it and then train new models w/ some example data before starting. Maybe just the target model? need to review the target model thing... 
		* might help avoid the absolute buttfuck stupidity at the beginning.

	* model memory
		* LSTM
		* Take in multiple 'frames' of history at each step


